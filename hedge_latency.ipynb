{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook to exemplify the Hedge_latencies. \n",
    "This notebook outlines latencies specific to our specific algorithm or autospreader. They depend on our TT server. <br>\n",
    "The definitions of the exchanges calculated here are inserted below\n",
    "-   Hedge Latency <br> \n",
    "    The time taken by our trading machine to send a hedge order after we receive a fill on the active leg of an autospreader.  It can be computed in this notebook by passing a csv to the process\\_csv function and then calling _hedge\\_latency function using the following: _hedge_latency(orders_df, ASELatency._hedge_latency_single_leg,\"hedge_latency\")<br>The parameters are: <br>\n",
    "    - orders_df: The dataframe of the AuditTrail.\n",
    "    - \"ASELatency._hedge_latency_single_leg: Function specifying what latency we compute, determines if we compute latency with acknowledgement or not. \n",
    "    - \"hedge_latency\": Specifies that we are considering hedge latency without acknowledgement. \n",
    "-   Hedge Latency with acknowledgement <br>\n",
    "    The time between when we receive a fill message, and the time we are acknowledged our new hedge order has arrived in the exchange. It can be computed in the notebook by passing a csv to the process\\_csv function and then calling \\_hedge\\_latency function using the following:  _hedge_latency(orders_df, ASELatency._hedge_latency_with_ack_time_single_leg, \"hedge_latency_with_ack\")<br> The parameters are: <br>\n",
    "    - orders_df: The dataframe of the AuditTrail.\n",
    "    - \"ASELatency._hedge_latency_with_ack_time_single_leg: Function specifying what latency we compute, determines if we compute latency with acknowledgement or not. \n",
    "    - \"hedge_latency_with_ack\": Specifies that we are considering hedge latency with acknowledgement. \n",
    "    <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from latency_config import (\n",
    "    ATColumns,\n",
    "    ETypes,\n",
    "    SynchMode,\n",
    "    QuotesColumns,\n",
    "    TradesColumns,\n",
    "    QUOTE_ORDERS,\n",
    "    ORDER_TYPE_TO_SIDE,\n",
    "    AT_TABLE_PKEYS,\n",
    ")\n",
    "from analysis_metrics.base.base_latency import BaseLatency\n",
    "from analysis_metrics.base.ase_latency import ASELatency\n",
    "import datetime as dt\n",
    "from datetime import timezone\n",
    "from audit_trail_manipulation import ATManipulation\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requote_latency(\n",
    "        cls,\n",
    "        liquid_leg_quotes: pd.DataFrame,\n",
    "        illiquid_leg_quotes: pd.DataFrame,\n",
    "        threshold=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes the time it takes to our logic to send a new quote after a market update. The market updates are our own quotes.\n",
    "        We are assuming we receive the acknowledge roughly at the same time we receive the market update, so we use the field `ATColumns.TIME_SENT`\n",
    "        of our own quotes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        liquid_leg_quotes : pd.DataFrame\n",
    "            Quotes that we send after reacting to the market\n",
    "        illiquid_leg_quotes : pd.DataFrame\n",
    "            Quotes we react to\n",
    "        threshold : int\n",
    "            Offsets larger than this will be dropped and won't be used, neither for statistics nor for plotting etc.\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Dataframe with \"requote_latency\", and timestamps of the illiquid and liquid legs.\n",
    "        \"\"\"\n",
    "        # mask_new = liquid_leg_quotes[ATColumns.EXECUTION_TYPE] == ETypes.NEW\n",
    "        requote_time = liquid_leg_quotes[ATColumns.EXCHANGE_TIME].copy()\n",
    "        # requote_time.loc[mask_new] = liquid_leg_quotes.loc[\n",
    "        #     mask_new, ATColumns.ORIGINAL_TIME\n",
    "        # ]\n",
    "        requote_time = requote_time.apply(lambda timestamp: timestamp.floor(\"ms\"))\n",
    "        received_time = illiquid_leg_quotes[ATColumns.TIME_SENT].apply(\n",
    "            lambda timestamp: timestamp.floor(\"ms\")\n",
    "        )\n",
    "        result = pd.DataFrame(\n",
    "            data={\n",
    "                \"requote_latency\": BaseLatency.to_millisec(\n",
    "                    requote_time - received_time.values\n",
    "                ).values,\n",
    "                \"illiquid_dtime\": illiquid_leg_quotes.index,\n",
    "                \"illiquid_row_id\": illiquid_leg_quotes[ATColumns.DF_ROW_ID].values,\n",
    "                \"liquid_dtime\": liquid_leg_quotes.index,\n",
    "                \"liquid_row_id\": liquid_leg_quotes[ATColumns.DF_ROW_ID].values,\n",
    "            },\n",
    "            index=liquid_leg_quotes.index,\n",
    "        )\n",
    "        if threshold is not None:\n",
    "            return result[result[\"requote_latency\"] <= threshold]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to compute the hedge latency\n",
    "def hedge_latency_single(  # TODO: fix for exchange-traded calendar/flies\n",
    "        \n",
    "        tt_parent_id: str,\n",
    "        ase_name: str,\n",
    "        latency_name: str,\n",
    "        df_group: pd.DataFrame,\n",
    "        latency_fn,\n",
    "    ):  # Already grouped by tt_parent_id\n",
    "        \"\"\"\n",
    "        Performs aggregation of DataFrame rows that have the same \"tt_parent_id\".\n",
    "        Parameters\n",
    "        ----------\n",
    "        tt_parent_id : str\n",
    "            Order id of the spread for which we want to compute the latencies.\n",
    "        spread_id : str\n",
    "            Name of the parent instrument\n",
    "        latency_name : str\n",
    "            Type of latency measure we are computing.\n",
    "        df_group : pd.DataFrame\n",
    "            Spread legs\n",
    "        latency_fn : (str, pd.DataFrame, pd.DataFrame) -> pd.DataFrame\n",
    "            Function that take latency description, active legs dataframe and hedge legs dataframe to compute latency\n",
    "\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with matched new orders and fills\n",
    "        \"\"\"\n",
    "        # separate orders in TRADES and NEWS\n",
    "        legs = df_group[ATColumns.INSTRUMENT].unique()\n",
    "        fills = df_group[df_group[ATColumns.EXECUTION_TYPE] == ETypes.TRADE]\n",
    "        new_orders = df_group[df_group[ATColumns.EXECUTION_TYPE] == ETypes.NEW]\n",
    "        if fills.empty:  # Exclude groups without trades\n",
    "            return None\n",
    "        # assume the first traded instrument is the active leg\n",
    "        active_leg = fills[ATColumns.INSTRUMENT].iloc[0]\n",
    "        active_leg_fills = fills[fills[ATColumns.INSTRUMENT] == active_leg]\n",
    "        # all the new orders that don't involve the active leg are hedge orders\n",
    "        hedge_new_orders = new_orders[new_orders[ATColumns.INSTRUMENT] != active_leg]\n",
    "        # for each hedge instrument we have a dataframe with the new orders of that instrument\n",
    "        hedge_legs_new_orders = [\n",
    "            group\n",
    "            for _, group in hedge_new_orders.groupby(\n",
    "                ATColumns.INSTRUMENT\n",
    "            )  # the version with [ATColumns.INSTRUMENT] gives a warning in pandas\n",
    "        ]\n",
    "        if hedge_new_orders.empty:  # Exclude groups without hedge orders\n",
    "            LOG.warn(f\"{tt_parent_id=} does not have any hedge orders in period\")\n",
    "            return None\n",
    "        # all the dataframes in the hedge_legs_new_orders list must have the same size as the dataframe with the active leg fills, i.e. the active leg\n",
    "        # and each hedge leg must have been traded with the same number of new orders\n",
    "        if any(\n",
    "            len(hedge_new_orders) != len(active_leg_fills)\n",
    "            for hedge_new_orders in hedge_legs_new_orders\n",
    "        ):\n",
    "            # NOTE: Here we simply import ASE module and then call _handle_unmatched_orders through that. we have removed \n",
    "            leg_latencies = ASELatency._handle_unmatched_orders(\n",
    "                active_leg_fills,\n",
    "                df_group,\n",
    "                tt_parent_id,\n",
    "                hedge_legs_new_orders,\n",
    "                latency_fn,\n",
    "                latency_name,\n",
    "                legs,\n",
    "            )\n",
    "            if leg_latencies == None:\n",
    "                LOG.warn(\n",
    "                    f\"WARN: {tt_parent_id=} has unmatching active fills and hedge orders. Legs={legs}\"\n",
    "                )\n",
    "                return None\n",
    "        else:\n",
    "            # if the times of the orders are too far apart we are doing a mismatch\n",
    "            # NOTE: We have imported ASELatency. \n",
    "            if ASELatency._distant_orders(\n",
    "                hedge_legs_new_orders, active_leg_fills, threshold=-10\n",
    "            ):\n",
    "                return None\n",
    "            # creates a list with dataframes each one containing a measurement of latency (by the latency function)\n",
    "            leg_latencies = [\n",
    "                latency_fn(latency_name, ase_name, active_leg_fills, leg_orders)\n",
    "                for leg_orders in hedge_legs_new_orders\n",
    "            ]\n",
    "        # concatenates the dataframes (on the index axis). In theory you can get a difference latency for each hedge leg\n",
    "        return ASELatency._combine_leg_latencies(leg_latencies, latency_name=latency_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _hedge_latency(df: pd.DataFrame, latency_fn, latency_name):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            AuditTrail data\n",
    "        latency_fn : (str, pd.DataFrame, pd.DataFrame) -> pd.DataFrame\n",
    "            Function measuring the latency (depends of course on the kind of latency being measured), this determines whether it is with or without acknowledgement\n",
    "        latency_name : str\n",
    "            The kind of latency being measured\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Hedge latencies\n",
    "        \"\"\"\n",
    "        hedge_latencies = df.groupby([ATColumns.TT_PARENT_ID]).apply(\n",
    "            lambda df_group: hedge_latency_single(\n",
    "                tt_parent_id=df_group.name,\n",
    "                ase_name=ASELatency._get_ase_name_from_order_id(df, df_group.name),\n",
    "                latency_name=latency_name,\n",
    "                df_group=df_group,\n",
    "                latency_fn=latency_fn,\n",
    "            )\n",
    "        )\n",
    "        if hedge_latencies.empty:\n",
    "            hedge_latencies[latency_name] = None\n",
    "\n",
    "        hedge_latencies = hedge_latencies.reset_index(level=[0]).dropna(\n",
    "            subset=latency_name\n",
    "        )\n",
    "        return hedge_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Comma separated file. \n",
    "def process_csv(csv_name):\n",
    "    orders_df = pd.read_csv(csv_name, index_col=\"dtime\")\n",
    "    orders_df[ATColumns.TIME_SENT]=orders_df.index\n",
    "    orders_df = BaseLatency.prepare_loaded(orders_df)\n",
    "    return orders_df\n",
    "orders_df = process_csv(\"raw_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtime\n",
      "2024-02-02 13:30:01.091000+00:00    0.0\n",
      "2024-02-02 13:30:01.092000+00:00    0.0\n",
      "2024-02-02 13:30:01.092000+00:00    0.0\n",
      "2024-02-02 13:30:01.092000+00:00    0.0\n",
      "2024-02-02 13:30:01.092000+00:00    0.0\n",
      "Name: hedge_latency, dtype: float64\n",
      "dtime\n",
      "2024-02-02 13:30:01.091000+00:00    12.0\n",
      "2024-02-02 13:30:01.092000+00:00    13.0\n",
      "2024-02-02 13:30:01.092000+00:00    13.0\n",
      "2024-02-02 13:30:01.092000+00:00    13.0\n",
      "2024-02-02 13:30:01.092000+00:00    13.0\n",
      "Name: hedge_latency_with_ack, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctardon\\AppData\\Local\\Temp\\ipykernel_13772\\2755470372.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hedge_latencies = df.groupby([ATColumns.TT_PARENT_ID]).apply(\n",
      "C:\\Users\\ctardon\\AppData\\Local\\Temp\\ipykernel_13772\\2755470372.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  hedge_latencies = df.groupby([ATColumns.TT_PARENT_ID]).apply(\n"
     ]
    }
   ],
   "source": [
    "hedge_latencies = _hedge_latency(orders_df, ASELatency._hedge_latency_single_leg,\"hedge_latency\")\n",
    "print(hedge_latencies['hedge_latency'])\n",
    "hedge_latencies_with_ack=_hedge_latency(\n",
    "            orders_df, ASELatency._hedge_latency_with_ack_time_single_leg, \"hedge_latency_with_ack\"\n",
    "        )\n",
    "print(hedge_latencies_with_ack['hedge_latency_with_ack'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
